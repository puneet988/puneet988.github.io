[{"authors":["admin"],"categories":null,"content":"I am a research scholar working in the field of cloud and aerosol modelling at the Centre for Atmospheric Sciences, IIT Delhi. My research interests include designing equilibrium and transient climate simulations, running the global climate model and statistically analyzing the model output together with various in-situ and satellite observations in order to understand various highlights and challenges in the model parametrization schemes.\nThis site is a personal blog. I will be posting blogs related to my work, some on programming and may be some random thoughts on life in general. And since I am also a big foodie, I am gonna share some of my food adventures too\u0026hellip; üòÄ\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a research scholar working in the field of cloud and aerosol modelling at the Centre for Atmospheric Sciences, IIT Delhi. My research interests include designing equilibrium and transient climate simulations, running the global climate model and statistically analyzing the model output together with various in-situ and satellite observations in order to understand various highlights and challenges in the model parametrization schemes.\nThis site is a personal blog. I will be posting blogs related to my work, some on programming and may be some random thoughts on life in general.","tags":null,"title":"Puneet Sharma","type":"authors"},{"authors":null,"categories":null,"content":"\nAfter spending a considerable time pursuing Ph.D. it felt like kind of stagnating with the daily routine. And many times it happens that you often feel like you should have taken a particular course taught by some faculty whose approach and teaching methodology appeals to you at a deeper level. So, I took this course on Earth System Modeling taught by Prof. Sandeep Sahany at my centre and in this course I had an opportunity of giving a talk on climate model evaluation which of course is strongly relevant to my Ph.D. too. Here I am sharing the slides of the talk which I gave. Also I was excited because it was the first time I used $\\rm \\LaTeX$ beamer template for making the slides which was really fun. üòÄ\nFeedbacks are welcome. üëç\n\u0026nbsp;\n  Understanding climate model evaluation and validation  from Puneet Sharma \n","date":1571158800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571158800,"objectID":"649397872b724d35d14b4b8b92335dc4","permalink":"/talk/first-talk/","publishdate":"2019-10-15T17:00:00Z","relpermalink":"/talk/first-talk/","section":"talk","summary":"Talk about climate model evaluation","tags":null,"title":"Understanding Climate Model Evaluation and Validation","type":"talk"},{"authors":[],"categories":["Machine Learning","Python"],"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  Here I am sharing the codes for deriving the number of clusters in K means clustering algorithm as shown by Bhavesh Bhatt in his youtube video.\n  \nimport math import matplotlib.pyplot as plt import matplotlib import numpy as np import seaborn as sns from sklearn.cluster import KMeans import warnings sns.set_color_codes() sns.set_context(\u0026quot;poster\u0026quot;) warnings.filterwarnings(\u0026quot;ignore\u0026quot;) np.random.seed(8) a = np.random.multivariate_normal([10,0],[[3,1],[1,4]], size=[100,]) b = np.random.multivariate_normal([0,20],[[3,1],[1,4]], size=[100,]) c = np.random.multivariate_normal([20,30],[[3,1],[1,4]], size=[100,]) X = np.concatenate((a,b,c)) print X.shape ## (300, 2) fig = plt.figure(figsize=(15, 10)) plt.xlim(-5,35) plt.ylim(-5,35) plt.scatter(X[:,0],X[:,1], c=\u0026#39;b\u0026#39;, s=5) dist_points_from_cluster_center = [] K = range(1,10) for no_of_clusters in K: k_model = KMeans(n_clusters=no_of_clusters) ; k_model.fit(X) ; dist_points_from_cluster_center.append(k_model.inertia_) ; print dist_points_from_cluster_center ## [70403.82188589728, 27407.17034387677, 2311.4033586287333, 1976.5540367939961, 1643.8735323124279, 1337.0526214543424, 1166.12005389885, 1036.87136535438, 942.9775249201616] fig = plt.figure(figsize=(15, 10)) plt.grid() plt.plot(K, dist_points_from_cluster_center) plt.xlabel(\u0026quot;No. of clusters K\u0026quot;) plt.ylabel(\u0026quot;Sum of squared distance\u0026quot;) fig = plt.figure(figsize=(15, 10)) plt.grid() plt.plot(K,dist_points_from_cluster_center) plt.plot([K[0], K[8]], [dist_points_from_cluster_center[0], dist_points_from_cluster_center[8]], \u0026#39;ro-\u0026#39;) plt.xlabel(\u0026quot;No. of clusters K\u0026quot;) plt.ylabel(\u0026quot;Sum of squared distance\u0026quot;) # Function to find distance # between a point and a line in 2-d def calc_distance(x1,y1,a,b,c): return abs((a*x1+b*y1+c)) / (math.sqrt(a*a+b*b)) Solving linear equation https://bobobobo.wordpress.com/2008/01/07/solving-linear-equations-ax-by-c-0/\na = dist_points_from_cluster_center[0] - dist_points_from_cluster_center[8] b = K[8] - K[0] c1 = K[0] * dist_points_from_cluster_center[8] c2 = K[8] * dist_points_from_cluster_center[0] c = c1-c2 distance_of_points_from_line = [] for k in range(9): distance_of_points_from_line.append( calc_distance(K[k], dist_points_from_cluster_center[k],a,b,c)) fig = plt.figure(figsize=(15, 10)) plt.grid() plt.plot(K, distance_of_points_from_line) print \u0026quot;Optimum value of k = \u0026quot; + str(distance_of_points_from_line.index(max(distance_of_points_from_line))+1) ## Optimum value of k = 3  ","date":1570060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569956913,"objectID":"84c0ad81966ba681605d4f58fdc5fac0","permalink":"/post/getting-reliable-k-in-k-means-clustering/","publishdate":"2019-10-03T00:00:00Z","relpermalink":"/post/getting-reliable-k-in-k-means-clustering/","section":"post","summary":"Instead of finding K in K means clustering through visualization, let us find K mathematically.","tags":["python","Statistics"],"title":"Getting reliable K in K means clustering","type":"post"},{"authors":[],"categories":["Python"],"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  In the previous post we discussed why and when we need to use Monte Carlo simulations.\nIn this post I will try to use Monte Carlo methods to perform time series forecasting of monthly cloud fraction available from the climate model CESM 1.2 - CAM 5.3 simulations over the Indian region.\nTime series forecasting is probably the most challenging and equally fruitful technique to make intelligent and scientific predictions from a time series data. (Larsen 2016; Ocean 2017). A time series data has many facets to it such as the trend, seasonality and some residue/remainder. These three components are usually there in case of a real-world data. We will talk about all these things in a separate post. So let‚Äôs start the analysis!!\nFirst we will read the data, average spatially over Central India and try to plot the daily time series of cloud fraction.\nimport cdms2, pandas, numpy, cdutil from calendar import isleap f = cdms2.open(\u0026#39;CLDTOT_AMIP_1979_2000_mon.nc\u0026#39;) data = f(\u0026#39;CLDTOT\u0026#39;) print data.shape ## (8030, 192, 288) Central India is approximately 20N to 28N and 74E to 86E.\ndata_cent_india = data(latitude=(20,28), longitude=(74,86)) Let us visualize one time slice to check the region.\nimport matplotlib.pyplot as plt import numpy as np from mpl_toolkits.basemap import Basemap, maskoceans import nclcmaps as ncm import cdms2 from matplotlib import ticker import matplotlib as mpl data_sel = data_cent_india(time=(\u0026quot;1980-08-15\u0026quot;,\u0026quot;1980-08-15\u0026quot;), squeeze=1) lat = data_sel.getLatitude() lon = data_sel.getLongitude() lons, lats = np.meshgrid(lon, lat) cmap = ncm.cmaps(\u0026#39;BlGrYeOrReVi200\u0026#39;) my_dpi = 96 vmin = 0. vmax = 1 fig = plt.figure(figsize=(1200./my_dpi, 800./my_dpi)) map1 = Basemap(resolution=\u0026#39;c\u0026#39;,projection=\u0026#39;cyl\u0026#39;,llcrnrlat=21.,urcrnrlat=27.,llcrnrlon=75.,urcrnrlon=85.) map1.drawparallels(np.arange(int(21),int(28),1),labels=[1,0,0,0], linewidth=0.0) map1.drawmeridians(np.arange(int(75),int(86),1),labels=[0,0,0,1], linewidth=0.0) datapc = map1.contourf(lons, lats, data_sel, vmin=vmin, vmax=vmax, cmap=cmap, latlon=True) fig.tight_layout(pad=6,w_pad=6, h_pad=8) v=np.arange(0,1.1,0.1) bounds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] norm = mpl.colors.BoundaryNorm(bounds, cmap.N) ax2 = fig.add_axes([0.05, 0.04, 0.9, 0.025], aspect=0.03) cb = mpl.colorbar.ColorbarBase(ax2, cmap=cmap, orientation=\u0026#39;horizontal\u0026#39;, norm=norm, spacing=\u0026#39;proportional\u0026#39;, ticks=v, format=\u0026quot;%.1f\u0026quot;) tick_locator = ticker.MaxNLocator(nbins=11) cb.locator = tick_locator cb.update_ticks() plt.show()  Cloud fraction over Central India 1980-08-15.  \nNow let us create the spatial average time series and visualize it.\nimport pandas,matplotlib import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib as mpl plt.rcParams[\u0026quot;legend.fontsize\u0026quot;] = 25 mpl.rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 20 mpl.rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 20 plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) dates = pandas.date_range(start=\u0026#39;1979-1-1\u0026#39;, end=\u0026#39;2000-12-31\u0026#39;, freq=\u0026#39;D\u0026#39;) leap = [] for each in dates: if each.month==2 and each.day ==29: leap.append(each) dates = dates.drop(leap) cf_time_series = cdutil.averager(data_cent_india, axis=\u0026#39;yx\u0026#39;) cf_series_numpy = numpy.array(cf_time_series) df_series = pandas.DataFrame(cf_series_numpy,index=dates) Let us plot 2 years of daily time series (1979-1980).\n df_sel = df_series[\u0026#39;1979-01-01\u0026#39;:\u0026#39;1980-12-31\u0026#39;] df_sel.columns = [\u0026#39;Cloud fraction\u0026#39;] #df_sel.plot(figsize=(25,10)) #plt.show() # convert date objects from pandas format to python datetime index = [pandas.to_datetime(date, format=\u0026#39;%Y-%m-%d\u0026#39;).date() for date in dates] ax = df_sel.plot(figsize=(25,10)) # set monthly locator ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2)) # set formatter ax.xaxis.set_major_formatter(mdates.DateFormatter(\u0026#39;%d-%m-%Y\u0026#39;)) ax.tick_params(\u0026#39;both\u0026#39;, length=20, width=2, which=\u0026#39;major\u0026#39;) # set font and rotation for date tick labels plt.gcf().autofmt_xdate() plt.show() From the time series, we see that there is a maximum of cloud cover over the Central India during June-July-August-September (JJAS) season which is the core monsoon season indicating high amount of deep clouds with larger spatial cover.\nTo use Monte Carlo methods, we will use a python package called prophet. This package uses Markov Chain Monte Carlo method to perform forecasting (Ocean 2017). For more information on the package, please refer to (Research, n.d.).\nimport pandas from fbprophet import Prophet import matplotlib.pyplot as plt import matplotlib as mpl mpl.rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 10 mpl.rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 10 plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) df_series.columns = [\u0026#39;Cloud Fraction\u0026#39;] df_series[\u0026#39;Days\u0026#39;] = dates print df_series.head(5) ## Cloud Fraction Days ## 1979-01-01 0.000000 1979-01-01 ## 1979-01-02 0.580835 1979-01-02 ## 1979-01-03 0.201555 1979-01-03 ## 1979-01-04 0.096733 1979-01-04 ## 1979-01-05 0.158427 1979-01-05 print df_series.dtypes ## Cloud Fraction float64 ## Days datetime64[ns] ## dtype: object  Prophet also imposes the strict condition that the input columns be named ds (the time column) and y (the metric column), so let‚Äôs rename the columns in our DataFrame:\n  - (Ocean 2017)\n  df_series = df_series.rename(columns={\u0026#39;Days\u0026#39;: \u0026#39;ds\u0026#39;, \u0026#39;Cloud Fraction\u0026#39;: \u0026#39;y\u0026#39;}) print df_series.head(5) ## y ds ## 1979-01-01 0.000000 1979-01-01 ## 1979-01-02 0.580835 1979-01-02 ## 1979-01-03 0.201555 1979-01-03 ## 1979-01-04 0.096733 1979-01-04 ## 1979-01-05 0.158427 1979-01-05 Let us visualize the forecast for the time series for next 5 years i.e.¬†from 2001-2005.\n # set the uncertainty interval to 95% (the Prophet default is 80%) my_model = Prophet(interval_width=0.95, daily_seasonality=True) my_model.fit(df_series) future_dates = my_model.make_future_dataframe(periods=60, freq=\u0026#39;MS\u0026#39;) future_dates.tail() forecast = my_model.predict(future_dates) print forecast[[\u0026#39;ds\u0026#39;, \u0026#39;yhat\u0026#39;, \u0026#39;yhat_lower\u0026#39;, \u0026#39;yhat_upper\u0026#39;]].head() my_model.plot(forecast,uncertainty=True) plt.show() Prophet also has the capability to analyse and present the components of the forecast such as trend and seasonality.\n my_model.plot_components(forecast) plt.show() \nReferences Larsen, Kim. 2016. ‚ÄúSorry Arima, but I‚Äôm Going Bayesian.‚Äù Multiple Hypothesis Testing | Stitch Fix Technology ‚Äì Multithreaded. https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/.\n Ocean, Digital. 2017. ‚ÄúA Guide to Time Series Forecasting with Prophet in Python 3.‚Äù SQLite Vs MySQL Vs PostgreSQL: A Comparison of Relational Database Management Systems | DigitalOcean. https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-prophet-in-python-3.\n Research, Facebook. n.d. ‚ÄúProphet: Forecasting at Scale.‚Äù Facebook Research. https://research.fb.com/prophet-forecasting-at-scale/.\n   ","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569956302,"objectID":"6e698c6d8bbb2b9b7717326883e95549","permalink":"/post/bootstrapping-monte-carlo-and-all-that-part2/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/post/bootstrapping-monte-carlo-and-all-that-part2/","section":"post","summary":"Second part of bootstrapping and monte carlo. Applying the theory through python and practical examples","tags":["Statistics","python"],"title":"Bootstrapping, Monte Carlo and all that‚Ä¶ ‚Äî Part2","type":"post"},{"authors":[],"categories":[],"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  For a recap of lists, vectors and matrices in R checkout Beginning with R ‚Äî The uncharted territory Part 1.\nTable of Contents  Arrays Factors Dataframes   Arrays Array is an object which can hold multidimensional data. Matrices are a subset of arrays as in they are two dimensional arrays. So, together with an attribute of dimension i.e. dim, arrays also have attribute dimnames. Array is simply a multidimensional data structure.\nIts syntax is a \u0026lt;- array(data, dim = c(x,y,z,t...))\na \u0026lt;- array(1:24, dim = c(3,4,2)); print(a) ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 13 16 19 22 ## [2,] 14 17 20 23 ## [3,] 15 18 21 24 vec1 \u0026lt;- c(10,20,30,40) vec2 \u0026lt;- c(12,13,14,15) b \u0026lt;- array(c(vec1,vec2), dim = c(2,2,2)); print(b) ## , , 1 ## ## [,1] [,2] ## [1,] 10 30 ## [2,] 20 40 ## ## , , 2 ## ## [,1] [,2] ## [1,] 12 14 ## [2,] 13 15 To define labels for different dimensions, use dimnames\nvec1 \u0026lt;- c(10,20,30,40) vec2 \u0026lt;- c(12,13,14,15) b \u0026lt;- array(c(vec1,vec2), dim=c(2,2,2), dimnames = list(c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;), c(\u0026quot;d\u0026quot;, \u0026quot;e\u0026quot;), c(\u0026quot;g\u0026quot;, \u0026quot;h\u0026quot;))); print(b) ## , , g ## ## d e ## a 10 30 ## b 20 40 ## ## , , h ## ## d e ## a 12 14 ## b 13 15 arr \u0026lt;- array(1:27,dim=c(3,3,3)); print(arr) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 10 13 16 ## [2,] 11 14 17 ## [3,] 12 15 18 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 19 22 25 ## [2,] 20 23 26 ## [3,] 21 24 27 t \u0026lt;- arr[1:2,1:2,,drop=FALSE]; print(attributes(t)); print(t) ## $dim ## [1] 2 2 3 ## , , 1 ## ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## ## , , 2 ## ## [,1] [,2] ## [1,] 10 13 ## [2,] 11 14 ## ## , , 3 ## ## [,1] [,2] ## [1,] 19 22 ## [2,] 20 23  Factors For the representation of categorical data, R has specific object called factors. Factors are basically integers and have labels associated with them. So, a particular number of factors are associated with a particular label. These labels are called levels. Factors look like characters but are integers in reality. Further uses of Factors are to sort all the categorical datasets according to one categorical dataset.\nfactor() command is used to create a factor object.\nfruits \u0026lt;- factor(c(\u0026#39;apple\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;apple\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;banana\u0026#39;,\u0026#39;apple\u0026#39;)) print(attributes(fruits)) ## $levels ## [1] \u0026quot;apple\u0026quot; \u0026quot;banana\u0026quot; \u0026quot;orange\u0026quot; ## ## $class ## [1] \u0026quot;factor\u0026quot; The levels are by default unordered. To order them you can define the levels.\nfruits \u0026lt;- factor(c(\u0026#39;apple\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;apple\u0026#39;,\u0026#39;orange\u0026#39;,\u0026#39;banana\u0026#39;,\u0026#39;apple\u0026#39;), levels = c(\u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;banana\u0026#39;)) print(attributes(fruits)) ## $levels ## [1] \u0026quot;apple\u0026quot; \u0026quot;orange\u0026quot; \u0026quot;banana\u0026quot; ## ## $class ## [1] \u0026quot;factor\u0026quot;  Dataframes Dataframes are used to store tabular data. Lists of equal length are stored in dataframes.\na \u0026lt;- data.frame(city=c(\u0026#39;Jaipur\u0026#39;,\u0026#39;Jammu\u0026#39;), rank = c(2,3)); print(a) ## city rank ## 1 Jaipur 2 ## 2 Jammu 3 The data stored can be of different type. One column may be character, another may be factors and so on. But each column must have same type of data.\n ","date":1563321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569925965,"objectID":"2358c8a3aa96f1f849952abbf4aa91fe","permalink":"/post/beginning-with-r-the-uncharted-territory-part-2/","publishdate":"2019-07-17T00:00:00Z","relpermalink":"/post/beginning-with-r-the-uncharted-territory-part-2/","section":"post","summary":"Second part of initial introduction to R. Topics include arrays, factors and dataframes.","tags":["R"],"title":"Beginning with R ‚Äî The uncharted territory Part 2","type":"post"},{"authors":[],"categories":null,"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  Coming from a non-programming background and Python being the first exposure to programming and data analysis, trying to get my hands dirty in R seemed pretty daunting at first. R at times can feel a bit peculiar and unique since it is based on the premise of doing data analysis and statistics rather than software programming which is the case with python. But as I push myself and try to learn the many quirks and leverages of R over python, it sort of gives a different perspective of doing data analysis. Plus, there is a strong edge of using R over python ‚Äî the vast and contemporary libraries of various statistical methodologies being implememted by statisticians world over.\nBesides its quirks, the most interesting IDE developed so far for R ‚Äî Rstudio, makes doing data analysis seem like fun activity. The various other things in Rstudio like making reports with support of \\(\\LaTeX\\) and HTML and making static websites using HUGO is something which makes life soooo easy.\nSo lets start the journey of R.\n\nTable of Contents  Introduction to R  R data types Handling undefined values Operators Data structures  Lists Vectors Matrices     Introduction to R R is a dynamic language developed largely for statistical computing.\nR data types Data type for a variable created in workspace is automatically assigned just like in Python.\na \u0026lt;- 4.2; b \u0026lt;- \u0026#39;Hello!\u0026#39; print(a); print(b) ## [1] 4.2 ## [1] \u0026quot;Hello!\u0026quot; To check the type of variable, use typeof()\nprint(typeof(a)); print(typeof(b)) ## [1] \u0026quot;double\u0026quot; ## [1] \u0026quot;character\u0026quot; Basic data types are:\n String/Character Number  Integer Double Complex  Boolean/Logical  A number whether integer or float is always represented as double.\na \u0026lt;- 20; print(typeof(a)) ## [1] \u0026quot;double\u0026quot; For explicit requirement of integer, add suffix L\nb \u0026lt;- 20L; print(typeof(b)) ## [1] \u0026quot;integer\u0026quot;  Handling undefined values Handling undefined/missing values is somewhat different than python. Python has only NaN values as undefined/missing values. In R, undefined values are basically represented using\n NULL NA NaN  All of three work differently.\nNULL which is a null object is used when there is no value present. If there is some value present in the vector or matrix and the value is not usable (fill_value), we use NA or NaN.\nNA or NaN are missing value indicator.\nprint(class(NULL)); print(class(NA)); print(class(NaN)) ## [1] \u0026quot;NULL\u0026quot; ## [1] \u0026quot;logical\u0026quot; ## [1] \u0026quot;numeric\u0026quot; NA comes when there is no TRUE or FALSE i.e.¬†logical indeterminacy. It can also come for missing value.\nNaN means 0/0\n Operators Mathematical operations are just like in Python.\n Multiplication * Division / Addition + Subtraction - Exponent ^ Modulus %% Integer Division %/%  Relational operators are same as in Python.\nLogical operators are as follows:\n Not ! Element wise AND \u0026amp; AND \u0026amp;\u0026amp; Element wise OR | OR || In the set %in%   Data structures In R there are 6 types of data structures\n Lists Vectors (or Atomic vectors) Matrices Arrays Factors Data Frames  Lists List in R can hold elements of different types. There is no coercion. A list can contain numeric, characters, boolean, matrices, vectors, arrays, lists stc.\nTo create list, use list() argument.\nlist_data \u0026lt;- list(\u0026#39;green\u0026#39;, \u0026#39;yellow\u0026#39;, 1, 2, c(4,5,6)) print(list_data) ## [[1]] ## [1] \u0026quot;green\u0026quot; ## ## [[2]] ## [1] \u0026quot;yellow\u0026quot; ## ## [[3]] ## [1] 1 ## ## [[4]] ## [1] 2 ## ## [[5]] ## [1] 4 5 6 To give names to each entry of list, use names() argument.\nnames(list_data) \u0026lt;- c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;E\u0026quot;) To access a particular entry in list use $\nprint(list_data$A); print(list_data$B) ### Access values of entries with name A and B ## [1] \u0026quot;green\u0026quot; ## [1] \u0026quot;yellow\u0026quot; print(list_data[1]) ### Access first label and its value ## $A ## [1] \u0026quot;green\u0026quot; print(list_data[[1]]) ### Access first value ## [1] \u0026quot;green\u0026quot; To merge two or more lists, use c()\na \u0026lt;- list(1,2,3,4); b \u0026lt;- list(5,6,7,8); c \u0026lt;- c(a,b); print(c) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 5 ## ## [[6]] ## [1] 6 ## ## [[7]] ## [1] 7 ## ## [[8]] ## [1] 8 Some predefined lists in R\nprint(letters); print(LETTERS); print(month.abb); print(month.name) ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot; \u0026quot;e\u0026quot; \u0026quot;f\u0026quot; \u0026quot;g\u0026quot; \u0026quot;h\u0026quot; \u0026quot;i\u0026quot; \u0026quot;j\u0026quot; \u0026quot;k\u0026quot; \u0026quot;l\u0026quot; \u0026quot;m\u0026quot; \u0026quot;n\u0026quot; \u0026quot;o\u0026quot; \u0026quot;p\u0026quot; \u0026quot;q\u0026quot; \u0026quot;r\u0026quot; \u0026quot;s\u0026quot; ## [20] \u0026quot;t\u0026quot; \u0026quot;u\u0026quot; \u0026quot;v\u0026quot; \u0026quot;w\u0026quot; \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; ## [1] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;C\u0026quot; \u0026quot;D\u0026quot; \u0026quot;E\u0026quot; \u0026quot;F\u0026quot; \u0026quot;G\u0026quot; \u0026quot;H\u0026quot; \u0026quot;I\u0026quot; \u0026quot;J\u0026quot; \u0026quot;K\u0026quot; \u0026quot;L\u0026quot; \u0026quot;M\u0026quot; \u0026quot;N\u0026quot; \u0026quot;O\u0026quot; \u0026quot;P\u0026quot; \u0026quot;Q\u0026quot; \u0026quot;R\u0026quot; \u0026quot;S\u0026quot; ## [20] \u0026quot;T\u0026quot; \u0026quot;U\u0026quot; \u0026quot;V\u0026quot; \u0026quot;W\u0026quot; \u0026quot;X\u0026quot; \u0026quot;Y\u0026quot; \u0026quot;Z\u0026quot; ## [1] \u0026quot;Jan\u0026quot; \u0026quot;Feb\u0026quot; \u0026quot;Mar\u0026quot; \u0026quot;Apr\u0026quot; \u0026quot;May\u0026quot; \u0026quot;Jun\u0026quot; \u0026quot;Jul\u0026quot; \u0026quot;Aug\u0026quot; \u0026quot;Sep\u0026quot; \u0026quot;Oct\u0026quot; \u0026quot;Nov\u0026quot; \u0026quot;Dec\u0026quot; ## [1] \u0026quot;January\u0026quot; \u0026quot;February\u0026quot; \u0026quot;March\u0026quot; \u0026quot;April\u0026quot; \u0026quot;May\u0026quot; \u0026quot;June\u0026quot; ## [7] \u0026quot;July\u0026quot; \u0026quot;August\u0026quot; \u0026quot;September\u0026quot; \u0026quot;October\u0026quot; \u0026quot;November\u0026quot; \u0026quot;December\u0026quot;  Vectors To create a vector, we use c() function. It basically concatenates things like a list in python.\nx \u0026lt;- c(1,2,3,4,5.4,\u0026#39;hello\u0026#39;,TRUE,FALSE); print(x) ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5.4\u0026quot; \u0026quot;hello\u0026quot; \u0026quot;TRUE\u0026quot; \u0026quot;FALSE\u0026quot; As we can see, a vector can have any data type, be it number, character or boolean. But we notice something. All the elements in the vector are coerced to character type because the vector contains a string \u0026quot;hello\u0026quot;. This is the effect of implicit coercion.\nFor strictly making a numeric vector, use vector() function.\nx \u0026lt;- vector(\u0026quot;numeric\u0026quot;, length = 20); print(x) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 We can use such a vector to preallocate a vector which can be used for appending values from a for loop which is faster than appending values to an empty vector since every time a value is appended in an empty vector, R makes a copy of it thus slowing the whole process.\nCoercion ‚Äî Objects like vectors, data frames etc. can be coerced to different classess using as.class function.\nx \u0026lt;- c(1,2,3,4); print(class(x)) ## [1] \u0026quot;numeric\u0026quot; y \u0026lt;- as.character(x); print(class(y)) ## [1] \u0026quot;character\u0026quot; y \u0026lt;- as.logical(x); print(class(y)) ## [1] \u0026quot;logical\u0026quot;  Matrices Matrix is same as a vector except it has an additional attribute of dimension. It has two dimensional data structure.\na \u0026lt;- matrix(c(6,2,6,8,3,2,6,8,0), nrow = 3, ncol = 3); print(a); print(attributes(a)) ## [,1] [,2] [,3] ## [1,] 6 8 6 ## [2,] 2 3 8 ## [3,] 6 2 0 ## $dim ## [1] 3 3 Matrices start filling row wise. Whereas in python, a matrix starts filling columnwise.\nIn R, we can pass the names of rows and columns.\na \u0026lt;- matrix(c(6,2,6,8,3,2,6,8,0), nrow = 3, ncol = 3, dimnames = list(c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;), c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;,\u0026#39;z\u0026#39;))); print(a) ## x y z ## a 6 8 6 ## b 2 3 8 ## c 6 2 0 print(colnames(a)); print(rownames(a)) ## [1] \u0026quot;x\u0026quot; \u0026quot;y\u0026quot; \u0026quot;z\u0026quot; ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; To access the elements of a matrix, use square brackets.\na1 \u0026lt;- matrix(c(6,2,6,8,3,2,6,8,0), nrow = 3, ncol = 3, dimnames = list(c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;), c(\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;,\u0026#39;z\u0026#39;))); print(a) ## x y z ## a 6 8 6 ## b 2 3 8 ## c 6 2 0 print(a1[2,2]) ### select 2nd row and 2nd column element  ## [1] 3 print(a1[c(2,3),c(1,2)]) ### select rows 2 and 3 and columns 1 and 2 ## x y ## b 2 3 ## c 6 2 But a[2,] (2nd row) or a[,2] (2nd column) gives a vector. To avoid this i.e.¬†to get a matrix, use drop = FALSE\nprint(a[2,]); print(dim(a[2,])) ## x y z ## 2 3 8 ## NULL print(a[2,,drop = FALSE]); print(dim(a[2,,drop = FALSE])) ## x y z ## b 2 3 8 ## [1] 1 3 Specific indexing can also be done.\nprint(a[c(1,2,4,6)]) ## [1] 6 2 8 2 You can also do indexing using logical vectors.\nprint(a[c(TRUE,FALSE,TRUE), c(TRUE,TRUE,FALSE)]) ## x y ## a 6 8 ## c 6 2 To transpose a matrix use t(a)\nTo combine vectors or matrices, use rbind or cbind\nDimension of matrix can also be changed (reshape)\ndim(a) \u0026lt;- c(1,9); print(dim(a)) ## [1] 1 9 Arrays, factors and data frames will be covered in next post.\n   ","date":1562025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569925058,"objectID":"62d7c709c174a06a97b9bb680d649922","permalink":"/post/beginning-with-r-the-uncharted-territory-part-1/","publishdate":"2019-07-02T00:00:00Z","relpermalink":"/post/beginning-with-r-the-uncharted-territory-part-1/","section":"post","summary":"Initial introduction to R. Topics include data types, data structures such as named lists, vectors and matrices.","tags":["R"],"title":"Beginning with R ‚Äî The uncharted territory Part 1","type":"post"},{"authors":["Charu Singh","Dilip Ganguly","**Puneet Sharma**","Shiwansha Mishra"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4a8e0ef287017fbf9ad0d1df23b2b2da","permalink":"/publication/2019-01-01_climate_response_of_/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019-01-01_climate_response_of_/","section":"publication","summary":"The equilibrium climate response of the south Asian monsoon (SAM) system to West Asia (WA), Tibetan Plateau (TP) and local dust emissions is investigated through a series of systematically designed idealized numerical experiments using CESM1-SOM, an atmospheric general circulation model coupled with a slab ocean model. Our results show that while an increased (decreased) dust emissions from the WA region produces a significant positive impact in the range of 0.20‚Äì0.40 mm/day (negative impact‚Äâ~‚Äâ0.40‚Äì0.60 mm/day) on the summer monsoon precipitation over south Asia particularly over the Indian subcontinent, a perpetual perturbation ranging from complete suppression to nearly doubling of dust emissions from the TP region or locally within south Asia yield relatively weaker and largely insignificant responses in the seasonal mean monsoon climate over south Asia. Distinctly opposite responses of the SAM system are noted to the suppression and enhancement of dust emissions from the WA region. A perpetual enhancement (suppression) of dust emissions from the WA region results in atmospheric circulation changes and alters the atmospheric temperature distribution over the south Asian region by warming (cooling) the free troposphere in such a way that it strengthens (weakens) the crucial north‚Äìsouth meridional mid-to-upper tropospheric temperature gradient as well as the magnitude of the vertical shear of zonal winds during the beginning of the summer monsoon season which further results in atmospheric circulation by favoring (opposing) the prevailing low level south-westerly winds and upper level northerly winds, leading to a strengthening (weakening) of the climatological summertime local Hadley cell circulation and leading to an increase (a decrease) in seasonal mean precipitation across most parts of South Asia and most significantly during the month of June.","tags":[],"title":"Climate response of the south Asian monsoon system to West Asia, Tibetan Plateau and local dust emissions","type":"publication"},{"authors":["Charu Singh","Dilip Ganguly","**Puneet Sharma**"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"01eb6b6e34383ab75ea386bf505fd5a5","permalink":"/publication/2019-01-01_impact_of_west_asia/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019-01-01_impact_of_west_asia/","section":"publication","summary":"In the present study, we examine the responses of South Asian Monsoon (SAM) rainfall at intra-seasonal scale to remote dust emissions from west Asia, Tibetan Plateau and local dust emissions from within south Asia using a state of the art coupled atmosphere-slab ocean model CESM1-SOM. A series of systematically designed idealized simulations are carried out in such a way that dust emissions from the selected source regions are either perpetually suppressed or enhanced from these source regions and the response of the intra-seasonal oscillations (ISOs) of SAM rainfall to such perturbations in dust emissions are investigated. It is noted that the intra-seasonal variability of SAM rainfall is dominated by three different ISOs with periodicities of 10‚Äì20 days, 30‚Äì60 days and 60‚Äì90 days. Modulations in the characteristics of each of these three ISOs are studied for each of the dust perturbation experiments. Statistically robust K‚ÄìS test and F test performed on the results from various dust perturbation experiments suggest that the perpetual perturbations in dust emissions from remote sources as well as locally from south Asia can significantly modulate the spatial and temporal structure of the ISOs of rainfall across scales during the monsoon season. Substantial changes are also noted in the spatial scales and propagation characteristics of ISOs attributable to the dust emission changes made over local and remote source regions in our idealized simulations. Our results suggest that perturbations in dust emissions over remote locations can substantially modulate the depth and duration of active and break rainfall events in the south Asian monsoon region. Results presented here have implications for better understanding and predicting the SAM rainfall variability at the intra-seasonal scales or at shorter time scales (~less than a season) under variable dust emissions from remote and local source regions.","tags":[],"title":"Impact of West Asia, Tibetan Plateau and local dust emissions on intra-seasonal oscillations of the South Asian monsoon rainfall","type":"publication"},{"authors":[],"categories":[],"content":" Extracting relevant results from a particular dataset requires a rigorous understanding of its statistical properties - mainly the type of distribution the data possesses. For some underlying statistical distribution in the data, parametric tests are applied to infer statistical properties of a particular estimator. It may also happen that the data does not possess any specific statistical distribution in which case, nonparametric tests are applied.\nIn statistical paradigm, we define an estimator to be a rule to conclude statistic of some unknown parameter from the data. For example, a mean is an estimator and mean of a sample from the dataset is statistic.\nAssuming that the data in hand is obtained for a variable following a certain measurement procedure, it will inherently possess some error - random error or systematic error - which will impart some uncertainty to various statistics we try to infer from the data. This uncertainty is basically an interval around derived statistic and defines the probability of interval of the true value of the statistic.\nNow, since the system from which the data is derived is not a fully deterministic system because of its inherent stochasticity, to make any predictions from the data we try to model it statistically. But the robustness of statistical model in terms of prediction is something which is highly desirable. This is where Monte Carlo simulations come to the rescue. Monte Carlo simulations are performed on the proposed statistical model to generate thousands of samples using some randomly generated data with known parameters (for example data randomly generated from a normal probability distribution function i.e.¬†normal PDF with known parameters like mean, median etc.) to estimate the characteristics of the sampling distribution. By doing this we get a range of estimates of a particular characteristics (let‚Äôs say median). This range of values of median for our statistical model together with the probability or likeliness of a particular value of median is crucial for understanding the robustness of our statistical model.\nThe method of bootstrapping is a special case of Monte Carlo simulation in which instead of generating random data from a distribution, sampling with replacement is performed from the population of available data (also known as historical data) regardless of its distribution to generate statistics of sampling distribution.\nIn essence, we can say that Monte Carlo simulation is essentially used to understand how well a particular estimator performs for a particular statistical model/distribution whereas bootstrapping is used to derive and understand the variability of a particular dataset in hand for which the distribution is unknown.\nIn the next post, we will try to apply and understand both the methods using some test data in order to realise the power of these two methods.\nTada!! üòÉ\n","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569924529,"objectID":"1f71f5264d86b1d11e44ea18b5175bed","permalink":"/post/bootstrapping-monte-carlo-and-all-that-part1/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/post/bootstrapping-monte-carlo-and-all-that-part1/","section":"post","summary":"Extracting relevant results from a particular dataset requires a rigorous understanding of its statistical properties - mainly the type of distribution the data possesses. For some underlying statistical distribution in the data, parametric tests are applied to infer statistical properties of a particular estimator. It may also happen that the data does not possess any specific statistical distribution in which case, nonparametric tests are applied.\nIn statistical paradigm, we define an estimator to be a rule to conclude statistic of some unknown parameter from the data.","tags":["Statistics"],"title":"Bootstrapping, Monte Carlo and all that‚Ä¶ ‚Äî Part1","type":"post"},{"authors":[],"categories":["Python"],"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  My first exposure to programming in my Ph.D.¬†came in the form of reading a bunch of netcdf files containing precipitation time series, averaging the data over time and visualize it spatially. Initially, I tried to use NetCDF Operators (NCO) and Climate Data Operators (CDO) to perform averaging or any other statistical analysis over the data, save the result in a seperate netcdf file, read the file in NCL and visualize it. But this whole procedure felt cumbersome and inefficient to me and whenever I did my analysis and visualization, I wished that somehow I could do all of it in the same programming environment!\nAs luck would have it, I found the solution to my predicament in the form of Python. The data reading and analysis packages such as cdms2, cdutil, genutil in CDAT together with the power of numpy and scipy and visualization packages such as PyNGL/PyNIO, matplotlib and seaborn streamlined my workflow in a very efficient way.\nBut for leveraging the power of these packages, it is necessary to first grasp the basic syntax and nuances of Python programming. Here I will share the basics of python and in the subsequent posts we will see how these packages can simplify the whole process of data analysis and visualization. Please be aware that syntax followed in this post and further posts will be based on python 3. Some syntax such as print statement or iterating over dictionary will differ for python 2 and python 3.\n\nTable of Contents  Python Introduction  Python jargons Data structures in python  List Tuple Dictionary     Python Introduction Python jargons Python is a dynamic language just like MATLAB. There is no need to define the type of any variable like we do in C or FORTRAN. For example\na = 2.0; b = \u0026#39;hello\u0026#39; print (\u0026#39;a = \u0026#39;,a, \u0026#39;and\u0026#39;, \u0026#39;b = \u0026#39;,b)  ## (\u0026#39;a = \u0026#39;, 2.0, \u0026#39;and\u0026#39;, \u0026#39;b = \u0026#39;, \u0026#39;hello\u0026#39;) Every variable in python has a datatype. We do not define the type of a variable. Python automatically assigns the type. At any moment, to check the data type of a variable use type function. For example\nprint (type(b)) ## \u0026lt;type \u0026#39;str\u0026#39;\u0026gt; Basic data types are\n String Number  Integer Long Float Complex  Set Boolean  Each variable has with it associated methods. Methods are the operations which can be operated on a variable. For example\nprint (b.split(\u0026#39;e\u0026#39;)) # Splits string into strings having characters before and after e ## [\u0026#39;h\u0026#39;, \u0026#39;llo\u0026#39;] Mathematical operations are just like in MATLAB. Basic mathematical operations are\nMultiplication * Division / Addition + Subtraction - Exponent **   Data structures in python Python has three important data structures\n List Tuple Dictionary  Use of each data structure will be explained further.\nList List is a data structure which can hold items regardless of their type. It can hold integers, strings, complex numbers etc. It can also hold many lists within itself. It is created by entering comma separated items in square brackets. For example :\na_list = [1,2,3,\u0026#39;hello\u0026#39;]; print (a_list) ## [1, 2, 3, \u0026#39;hello\u0026#39;] Check the methods associated with the list by typing dir(a_list).\nBefore going further we talk about mutable and immutable data structures.\n Mutable data structures are those data structures which can be altered in place. Which means we can delete an item, replace an item, alter an item, append an item inside that data structure.\n Immutable data structures are those data structures which can not be altered in place. We can make copy of it with the changes to a different variable but we can not make any in place changes.\n  We will further look at examples to understand this.\n Tuple Tuple is also a data structure which can hold items regardless of their type just like a list. It is created by entering comma separated items in parenthesis. For example :\na_tuple = (1,2,3,\u0026#39;hello\u0026#39;,1.2); print (a_tuple) ## (1, 2, 3, \u0026#39;hello\u0026#39;, 1.2) Again check the methods associated with the tuple by typing dir(a_tuple). You will find that some methods available for list like pop, delete, append etc are missing for tuple. This is because a tuple is immutable. To change a tuple you need to create another tuple with the changes.\na_list.append(10) print (a_list) ## [1, 2, 3, \u0026#39;hello\u0026#39;, 10] If you try a_tuple.append(10), it will throw up error because tuple is an immutable object.\nprint (a_tuple) ## (1, 2, 3, \u0026#39;hello\u0026#39;, 1.2) new_tuple = (10,11) print (new_tuple) ## (10, 11) final_tuple = a_tuple + new_tuple print (final_tuple) ## (1, 2, 3, \u0026#39;hello\u0026#39;, 1.2, 10, 11) The important difference between a list and a tuple is that all things being same, a list is a mutable container of items, a tuple on the other hand is immutable.\nFor this very reason, iterating over a tuple is always faster than iterating over a list.\nWe will talk about iterating over list, tuple and dictionary later.\n Dictionary Dictionary is the most versatile and probably the most helpful data structure in python.\nDictionary is also a mutable container of items but in dictionary we can assign each item or a list/tuple of items a key. A key acts as an identifier to a particular item or a list of items. A dictionary is created by entering comma separated key value pairs in curly brackets. Key value pairs hold a colon between them. For example :\na_dict = {\u0026#39;Name\u0026#39;:\u0026#39;Puneet\u0026#39;, \u0026#39;Date of Birth\u0026#39;:6, \u0026#39;Month of Birth\u0026#39;:11, \u0026#39;Year of Birth\u0026#39;:1988, \u0026#39;Lab mates\u0026#39;: [\u0026#39;Ram\u0026#39;,\u0026#39;Amit\u0026#39;,\u0026#39;Satya\u0026#39;]} print (a_dict.keys()) ## [\u0026#39;Month of Birth\u0026#39;, \u0026#39;Date of Birth\u0026#39;, \u0026#39;Year of Birth\u0026#39;, \u0026#39;Lab mates\u0026#39;, \u0026#39;Name\u0026#39;] print (a_dict.values()) ## [11, 6, 1988, [\u0026#39;Ram\u0026#39;, \u0026#39;Amit\u0026#39;, \u0026#39;Satya\u0026#39;], \u0026#39;Puneet\u0026#39;] You will see that, once the dictionary is created, the order of printed items (keys) is not same as the order of entered items. But no need to worry! You can always access any item with its key.\nprint (a_dict[\u0026#39;Name\u0026#39;]) ## Puneet Please note that a dictionary is a mutable data structure.\nSubsetting, iterating over data structures and different control statements will be covered in the next post.\nPhew!! That was a lot to talk about. I need a drink.\n   ","date":1540598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569910945,"objectID":"ffc10c4fbe6b36b718109d2e0ea76b0c","permalink":"/post/python-the-swiss-army-knife-part-1/","publishdate":"2018-10-27T00:00:00Z","relpermalink":"/post/python-the-swiss-army-knife-part-1/","section":"post","summary":"My journey with python in two part series. Topics covered include lists, tuple and dictionary.","tags":["python"],"title":"Python ‚Äî The swiss army knife Part 1","type":"post"},{"authors":[],"categories":["Python"],"content":"  pre { overflow-x: auto; } pre code { word-wrap: normal; white-space: pre; }  For a quick recap:\n List  a_list = [1,2,3,4,‚Äòhello‚Äô]  Tuple  a_tuple = (1,2,3,4,‚Äòhello‚Äô)  Dictionary  a_dict = {‚ÄòName‚Äô:‚ÄòPuneet‚Äô, ‚ÄòAge‚Äô:28}   A tuple is an immutable data structure while list and dictionary are mutable data structure.\n\nTable of Contents  Slicing a list or a tuple Iterating over a list, tuple and dictionary Some helpful constructs in python  Enumerate Zip  Conditional statements  if-else if-elif-else while  Break and Continue Range function   Slicing a list or a tuple Slicing and subsetting a list or a tuple is same as selecting characters from a string. For example:\na = [\u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;anthony\u0026#39;, \u0026#39;gonsalves\u0026#39;] print(a[0], a[-1], a[:-1], a[1:3], sep=\u0026#39;\\n\u0026#39;) ## my ## gonsalves ## [\u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;anthony\u0026#39;] ## [\u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;] b = (1,2,3,4,5,6) print (b[-1], b[2:4], sep=\u0026#39;\\n\u0026#39;) ## 6 ## (3, 4) A list/tuple can contain many lists/tuples as in nested lists/tuples.A list can contain tuples as well as a tuple can contain many lists. Por ejemplo\nnested_tuple = (1,2,3, [6,7]) nested_list = [1,2,[5,6,7],[8,9]] print (nested_list[2], nested_list[1:3], sep=\u0026#39;\\n\u0026#39;) ## [5, 6, 7] ## [2, [5, 6, 7]]  Iterating over a list, tuple and dictionary Iterating over items in a list, tuple or dictionary is achieved using for loop. For example\nlist_of_fruits = [\u0026#39;orange\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;papaya\u0026#39;] for i in list_of_fruits: print (i) ## orange ## banana ## apple ## papaya Similarly for tuples\ntuple_of_students = (\u0026#39;ravi\u0026#39;, \u0026#39;jack\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;ronald\u0026#39;) for i in tuple_of_students: print (i) ## ravi ## jack ## ram ## ronald To iterate over a dictionary,\ndict_of_fruits = {\u0026#39;Apple\u0026#39;:3, \u0026#39;Orange\u0026#39;:2, \u0026#39;Banana\u0026#39;:8} for i, j in dict_of_fruits.items(): print (i,j) ## Apple 3 ## Orange 2 ## Banana 8 There are many more helpful iteration constructs which can be really fast and helpful.\nNotice that whenever we write iteration constructs or control statements, we end the statement with a colon and indent the code. Indenting the code in python is forced. Without indenting, the code will throw up error. In principle, decide for one rule of indenting and follow it in all your coding statements. Indenting can be done for 1, 2 or any number of spaces or tab. Generally 4 space rule is followed. This forced indenting makes the code look cleaner and more readable.\n Some helpful constructs in python Enumerate enumerate is helpful when you want to iterate over a sequence of list/tuple/dictionary/string such that you wish to access both the index and the item at the same time. For example\nlist_of_fruits = [\u0026#39;Apple\u0026#39;,\u0026#39;Orange\u0026#39;,\u0026#39;Banana\u0026#39;,\u0026#39;Grapes\u0026#39;,\u0026#39;Watermelon\u0026#39;] for i, val in enumerate(list_of_fruits): print (i,val) ## 0 Apple ## 1 Orange ## 2 Banana ## 3 Grapes ## 4 Watermelon  Zip zip function is helpful when you want to simultaneously iterate over multiple lists/tuples/dictionaries\nme = [1,2,3,4] you = [5,6,7,8,9] for i, j in zip(me,you): print (i,j) ## 1 5 ## 2 6 ## 3 7 ## 4 8 Notice that if list/tuple/dictionaries are not of same length then the smaller length is iterated\nThere are many functions available in python which are helpful in many ways.\n a = [1,3,5,2,4] print (len(a), min(a), max(a)) ## 5 1 5   Conditional statements if-else a = [1,2,3,4] for i in a: if i\u0026gt;2: print (i) else: print (\u0026#39;digit less than 2\u0026#39;) ## digit less than 2 ## digit less than 2 ## 3 ## 4  if-elif-else a = [1,2,3,4,5,6,7,8,9] for i in a: if i\u0026lt;3: print (\u0026#39;less than 3\u0026#39;) elif i\u0026lt;6: print (\u0026#39;less than 6\u0026#39;) else: print (i) ## less than 3 ## less than 3 ## less than 6 ## less than 6 ## less than 6 ## 6 ## 7 ## 8 ## 9  while x = 0 while x\u0026lt;5: print (x) x = x + 1 ## 0 ## 1 ## 2 ## 3 ## 4   Break and Continue Sometimes we need to stop the iteration if a condition is met and break out of the loop. Then we use break statement.\na = [10,14,15,18,19,21,25,28] for i, val in enumerate(a): if i \u0026gt; 3: break print (val) ## 10 ## 14 ## 15 ## 18 Sometimes during the iteration we want that if a condition is satisfied, the operation within the loop is skipped. Then we use continue statement.\na = [1,2,3,4,6,8,9,10] for val in a: if val%2==0: continue print (val) ## 1 ## 3 ## 9  Range function import numpy arr = numpy.arange(10) for i in range(len(arr)): print (i) ## 0 ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 ## 7 ## 8 ## 9 a = [1,2,3,4,5,6,7,8,9,10,11,12,13,14] for i in range(0,len(a),3): print (a[i]) ## 1 ## 4 ## 7 ## 10 ## 13  ","date":1540598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569921760,"objectID":"43dad2a9ef72203e67edca278b3acf6b","permalink":"/post/python-the-swiss-army-knife-part-2/","publishdate":"2018-10-27T00:00:00Z","relpermalink":"/post/python-the-swiss-army-knife-part-2/","section":"post","summary":"Second part of my journey with python. Topics covered subsetting over lists, tuple and dictionary. Also included are conditional constructs like if-else, if-elif-else and while. Brief idea about enumerate and zip function is also included.","tags":["python"],"title":"Python ‚Äî The swiss army knife Part 2","type":"post"},{"authors":[],"categories":["Python"],"content":" People working in the field of atmospheric science and climate change often have to deal with data stored in a netCDF file format.\nNetCDF file has an extension .nc and is used as a container of gridded data of different dimensions, variables and their associated attributes. Usually netCDF classic format is used to store the climate data.\nHere we will use two different packages of python to access a model output in netcdf file format and access its variables.\nWe will be using the file which you can download from the following link. CAM_run_2003_01_daily_data.nc\n\nCDAT way CDAT is a set of different python packages which together form a complete environment for climate data exploration and its visualization. It can access data in netCDF, HDF4 and grib data formats. For visualizing the results it includes packages such as VCS and matplotlib. To install CDAT using Conda run : conda create -n cdat8 -c cdat/label/v80 -c conda-forge -c cdat python=2.7 cdat\n#--- Import the cdms2 module import cdms2  #--- Load the netcdf file f = cdms2.open(\u0026#39;CAM_run_2003_01_daily_data.nc\u0026#39;) #--- Store the variable names in a list variable_names = f.listvariables() variable_names.sort() print (variable_names) ## [\u0026#39;CLDTOT\u0026#39;, \u0026#39;CLOUD\u0026#39;, \u0026#39;P0\u0026#39;, \u0026#39;PS\u0026#39;, \u0026#39;hyam\u0026#39;, \u0026#39;hybm\u0026#39;, \u0026#39;time_bnds\u0026#39;] #--- Read the variable CLOUD CLOUD_data = f(\u0026#39;CLOUD\u0026#39;) print (CLOUD_data.shape) ## (31, 30, 192, 288) Here (31,30,192,288) is the shape of the data where the first dimension is time, second dimension is levels, third dimension is latitude and fourth dimension is longitude.\nTo know more about CDAT, follow the documentation\n¬† NetCDF4-python way To install netcdf4-python using Conda run : conda install -c conda-forge netcdf4\n#--- Import the netCDF4 module import netCDF4  #--- Load the netcdf file f = netCDF4.Dataset(\u0026#39;CAM_run_2003_01_daily_data.nc\u0026#39;) #--- Store the variable names in a list variable_names = f.variables.keys() # variables in unicode variable_names = [x.encode(\u0026#39;UTF8\u0026#39;) for x in variable_names] # encode to string print (variable_names)  ## [\u0026#39;CLDTOT\u0026#39;, \u0026#39;CLOUD\u0026#39;, \u0026#39;P0\u0026#39;, \u0026#39;PS\u0026#39;, \u0026#39;hyam\u0026#39;, \u0026#39;hybm\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;lev\u0026#39;, \u0026#39;lon\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;time_bnds\u0026#39;] #--- Read the variable CLOUD CLOUD_data = f.variables[\u0026#39;CLOUD\u0026#39;] print (CLOUD_data.shape) ## (31, 30, 192, 288) To know more about netcdf4-python, follow the documentation\n ","date":1540425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569862642,"objectID":"bc1d19deb65ed97c2f7885e108cb1a54","permalink":"/post/juggling-netcdf-file-in-cdat-and-netcdf4-python/","publishdate":"2018-10-25T00:00:00Z","relpermalink":"/post/juggling-netcdf-file-in-cdat-and-netcdf4-python/","section":"post","summary":"People working in the field of atmospheric science and climate change often have to deal with data stored in a netCDF file format.\nNetCDF file has an extension .nc and is used as a container of gridded data of different dimensions, variables and their associated attributes. Usually netCDF classic format is used to store the climate data.\nHere we will use two different packages of python to access a model output in netcdf file format and access its variables.","tags":["python","netcdf","climate data"],"title":"Juggling NetCDF file in CDAT and NetCDF4-python","type":"post"},{"authors":null,"categories":[],"content":"  Writing is hard work. A clear sentence is no accident. Very few sentences come out right the first time, or even the third time. Remember this in moments of despair. If you find that writing is hard, it‚Äôs because it is hard.\n  - William Zinsser, On Writing Well\n These lines have always resonated in my mind whenever I sat down to write a technical article or some random thoughts circling in my mind for that matter.\nWelcome to my first blog post!. I am really excited to start this journey. Blogs have always fascinated me more so because while writing a blog, one can be as free and candid as one wants when it comes to jotting down the thoughts in a coherent manner. Blogs are also a way to share your accumulated knowledge whether it is technical or some general stuff in an organised way.\nTruth is I am no excellent writer, but with time I expect to share my thoughts and technical findings related to my field of research and statistics and grow with the scientific community.\nHappy blogging !! üòÉ\n\nP.S. To setup the blogging environment I followed the instructions mentioned on Building a Blog with Blogdown and GitHUb\n","date":1536624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536624000,"objectID":"d421e15d617a1d1bcf2550509b5903d0","permalink":"/post/first-blog/","publishdate":"2018-09-11T00:00:00Z","relpermalink":"/post/first-blog/","section":"post","summary":"Writing is hard work. A clear sentence is no accident. Very few sentences come out right the first time, or even the third time. Remember this in moments of despair. If you find that writing is hard, it‚Äôs because it is hard.\n  - William Zinsser, On Writing Well\n These lines have always resonated in my mind whenever I sat down to write a technical article or some random thoughts circling in my mind for that matter.","tags":["academic","hugo"],"title":"First blog","type":"post"},{"authors":["**Puneet Sharma**","Dilip Ganguly"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"cde00eab160febdd41a3cd3806570317","permalink":"/publication/2018-01-01_aerosol-cloud_inter/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/2018-01-01_aerosol-cloud_inter/","section":"publication","summary":"Critical evaluation of complex parametrization schemes such as for aerosol‚Äìcloud interaction (ACI) implemented in global climate models (GCMs) is crucial not only for model improvement purposes but also for reducing the uncertainties associated with climate change projections made using these models. In this work, we evaluate the performance of the atmospheric component of the Community Earth System Model, that is the Community Atmosphere Model version 5 (CESM1-CAM5) in simulating aerosols, clouds and their interactions using the Moderate Resolution Imaging Spectroradiometer (MODIS) Aqua satellite observations over the northern Bay of Bengal region. Quantification of total indirect effect (TIE) and its decomposition into first indirect effect (FIE) and second indirect effect (SIE) based on statistical relationships of clouds and aerosols are examined for liquid clouds and aerosol parameters from the GCM and satellite observations for a period of 5 years (2006-2010) over the northern Bay of Bengal (BoB) for winter season (DJF). The season of this study is when the prevailing surface level wind flow is predominantly from polluted Indo-Gangetic basin toward the ocean and characterized by high aerosol loading as well as dominance of stratocumulus and cumulus clouds, thereby providing ideal conditions for understanding ACI. Two sets of simulations, one with predicted meteorology and the other with prescribed meteorology based on MERRA-2 for the same period are carried out in order to ascertain the impact of meteorology on the sensitivity relationships between cloud and aerosol variables. CFMIP Observations Simulator Package (COSP) output for cloud variables is used to make the comparison more robust. Simulation of cloud fraction with prescribed meteorology shows high agreement with the observations compared to the simulation results involving predicted meteorology indicating some deficiencies in the model simulated meteorology. Aerosol optical depth (œÑ$_{a}$) is underestimated in model simulation by almost a factor of 2 when compared with observation suggesting a compelling need for improving the aerosol inventories over the region. CAM5 simulates mostly cumulus clouds during the study period whereas both cumulus and stratocumulus clouds are clearly evident in the observations, which may contribute to the bias in ACI between model and observations. Cloud droplet number concentration (Nc) computed using identical formulation applied on model simulations and observation is noted to be higher in model simulations than in observations revealing a stronger ACI in model simulations. TIE, FIE, and SIE simulated by CAM5 using prescribed meteorology are comparatively closer to observations than in the simulation involving predicted meteorology indicating that the confounding influence of model simulated meteorological quantities on aerosol-cloud relationship in CAM5 needs to be resolved through more detailed assessment of ACI for making any realistic assessment of the impacts of aerosols on climate not just over South Asia but also globally. More results with greater details will be presented.","tags":[],"title":"Aerosol-Cloud interaction over the Bay of Bengal during polluted winter season: A modelling perspective.","type":"publication"}]